{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Links to refer to\n",
    "* [Dataset Location - Select the PAF Prediction Challenge Database (afpbd)](https://physionet.org/cgi-bin/atm/ATM \"PhysioBank ATM\")\n",
    "* [Refer this to see how data is supposed to be fed into the network for a similar kind of problem](https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0 \"Human Activity Recognition (HAR) Tutorial with Keras and Core ML\")\n",
    "* [This is the implementation of the above with 1D Convolution](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf \"Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences\")\n",
    "* [Choice of Optimizers available in Keras](https://keras.io/optimizers \"Keras Optimizers\")\n",
    "* [Choice of Loss Functions available in Keras](https://keras.io/losses \"Keras Loss Functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameter to plot at 30x15 size\n",
    "plt.rcParams['figure.figsize'] = 30, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the locations of the files\n",
    "root_folder_train = 'C:\\\\Users\\\\animi\\\\Documents\\\\Dataset\\\\ECG Dataset\\\\Train\\\\'\n",
    "root_folder_test = 'C:\\\\Users\\\\animi\\\\Documents\\\\Dataset\\\\ECG Dataset\\\\Test\\\\'\n",
    "category = ['Normal', 'Abnormal']\n",
    "destination = 'C:\\\\Users\\\\animi\\\\Documents\\\\Dataset\\\\ECG Dataset\\\\Customized_Data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset and do the required formatting to be able to feed the data into the Keras Model\n",
    "def load_data(root_folder):\n",
    "    # These are delcarations of variables that have been used inside the for loop\n",
    "    final_list = list()\n",
    "    labels = list()\n",
    "    # It is iterating through both the categories Normal and Abnormal\n",
    "    for cat in category:\n",
    "        # It is taking and processing each file in the folder\n",
    "        for filename in os.listdir(root_folder+cat):\n",
    "            \n",
    "            # Read each file for each category and drop the unnecessary columns\n",
    "            path = root_folder + cat + '\\\\' + filename\n",
    "            df = pd.read_csv(path) # Read the CSV using inbuilt Pandas Function\n",
    "            df.drop(index=0, axis=0, inplace=True) # Drop the first row, which contains the units of measurement (useless for our use case)\n",
    "            df.columns = [\"time\", \"ECG0\", \"ECG1\"] # Rename the columns for convinience and easy access of the columns\n",
    "            df.drop(['time'], axis=1, inplace=True) # Drop the time column, as we are not using it as a time series. We are using the indexes instead\n",
    "            df.ECG0 = pd.to_numeric(df.ECG0) # The data by default is in the form of an object, Convert each row into numeric or floating point\n",
    "            df.ECG1 = pd.to_numeric(df.ECG1)\n",
    "            \n",
    "            print(filename, len(df))\n",
    "            \n",
    "            # Split each file into 6 parts and then make each of them a new row by transposing\n",
    "            df_split = np.array_split(df, 30) # Split the dataset into 30 different sets. This is not mandatory, but is suggested since the dataset size is less\n",
    "            for splitted_array in df_split:\n",
    "                final_list.append(np.array(splitted_array)) # After splitting, we are appending all the splitted arrays into 1 single large array of 3 dimentions\n",
    "                # The following if-else block is used to create labels. We have taken '1' for AF ECG and '0' for Normal ECG\n",
    "                # This is not the ideal way to create labels, but this is the most simplest way for this situation\n",
    "                if cat == 'Normal':\n",
    "                    labels.append(0)\n",
    "                if cat == 'Abnormal':\n",
    "                    labels.append(1)\n",
    "\n",
    "    # Before returning, convert the lists to arrays and increase the dmentions for being able to feed into the Neural Network\n",
    "    return np.array(final_list), np.expand_dims(np.array(labels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n01.csv 7680\n",
      "n02.csv 7680\n",
      "n03.csv 38400\n",
      "n04.csv 7680\n",
      "n05.csv 7680\n",
      "n06.csv 7680\n",
      "n07.csv 7680\n",
      "n08.csv 7680\n",
      "n09.csv 7680\n",
      "n10.csv 7680\n",
      "n11.csv 7680\n",
      "n12.csv 38400\n",
      "n13.csv 38400\n",
      "n14.csv 7680\n",
      "n15.csv 7680\n",
      "n16.csv 7680\n",
      "n17.csv 7680\n",
      "n18.csv 7680\n",
      "n19.csv 7680\n",
      "n20.csv 7680\n",
      "n21.csv 7680\n",
      "n22.csv 7680\n",
      "n23.csv 7680\n",
      "n24.csv 7680\n",
      "n25.csv 7680\n",
      "n26.csv 7680\n",
      "n27.csv 7680\n",
      "n28.csv 7680\n",
      "n29.csv 7680\n",
      "n30.csv 7680\n",
      "n31.csv 7680\n",
      "n32.csv 7680\n",
      "n33.csv 7680\n",
      "n34.csv 7680\n",
      "n35.csv 7680\n",
      "n36.csv 7680\n",
      "n37.csv 7680\n",
      "n38.csv 7680\n",
      "n39.csv 7680\n",
      "n40.csv 7680\n",
      "n41.csv 7680\n",
      "n42.csv 7680\n",
      "n43.csv 7680\n",
      "n44.csv 7680\n",
      "n45.csv 7680\n",
      "n46.csv 7680\n",
      "n47.csv 7680\n",
      "n48.csv 7680\n",
      "n49.csv 7680\n",
      "n50.csv 7680\n",
      "p01.csv 7680\n",
      "p02.csv 7680\n",
      "p03.csv 7680\n",
      "p04.csv 7680\n",
      "p05.csv 7680\n",
      "p06.csv 7680\n",
      "p07.csv 7680\n",
      "p08.csv 7680\n",
      "p09.csv 7680\n",
      "p10.csv 7680\n",
      "p11.csv 7680\n",
      "p12.csv 7680\n",
      "p13.csv 7680\n",
      "p14.csv 7680\n",
      "p15.csv 7680\n",
      "p16.csv 7680\n",
      "p17.csv 7680\n",
      "p18.csv 7680\n",
      "p19.csv 7680\n",
      "p20.csv 7680\n",
      "p21.csv 7680\n",
      "p22.csv 7680\n",
      "p23.csv 7680\n",
      "p24.csv 7680\n",
      "p25.csv 7680\n",
      "p26.csv 7680\n",
      "p27.csv 7680\n",
      "p28.csv 7680\n",
      "p29.csv 7680\n",
      "p30.csv 7680\n",
      "p31.csv 7680\n",
      "p32.csv 7680\n",
      "p33.csv 7680\n",
      "p34.csv 7680\n",
      "p35.csv 7680\n",
      "p36.csv 7680\n",
      "p37.csv 7680\n",
      "p38.csv 7680\n",
      "p39.csv 7680\n",
      "p40.csv 7680\n",
      "p41.csv 7680\n",
      "p42.csv 7680\n",
      "p43.csv 7680\n",
      "p44.csv 7680\n",
      "p45.csv 7680\n",
      "p46.csv 7680\n",
      "p47.csv 7680\n",
      "p48.csv 7680\n",
      "p49.csv 7680\n",
      "p50.csv 7680\n",
      "samples (1).csv 7680\n",
      "samples (2).csv 7680\n",
      "samples.csv 7680\n",
      "samples (1).csv 7680\n",
      "samples (2).csv 7680\n",
      "samples.csv 7680\n",
      "(3000,) (3000, 1)\n",
      "(180, 256, 2) (180, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the training and testing dataset separately by calling the function for each of their root folder locations\n",
    "X_train, y_train = load_data(root_folder_train)\n",
    "X_test, y_test = load_data(root_folder_test)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the formatted data for easy access later. This is then loaded and used for the neural network\n",
    "np.save('C:/Users/animi/Documents/Dataset/ECG Dataset/Customized_Data/X_train.npy', X_train)\n",
    "np.save('C:/Users/animi/Documents/Dataset/ECG Dataset/Customized_Data/y_train.npy', y_train)\n",
    "np.save('C:/Users/animi/Documents/Dataset/ECG Dataset/Customized_Data/X_test.npy', X_test)\n",
    "np.save('C:/Users/animi/Documents/Dataset/ECG Dataset/Customized_Data/y_test.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
